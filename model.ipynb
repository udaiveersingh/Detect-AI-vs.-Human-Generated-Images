{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91198,"databundleVersionId":10884264,"sourceType":"competition"},{"sourceId":10550636,"sourceType":"datasetVersion","datasetId":6412205}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/udaiveersinghsdjab/efficientnetb0-tensorflow-transfer-learning?scriptVersionId=235080501\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:38:28.728146Z","iopub.execute_input":"2025-04-05T17:38:28.728348Z","iopub.status.idle":"2025-04-05T17:39:09.759263Z","shell.execute_reply.started":"2025-04-05T17:38:28.728329Z","shell.execute_reply":"2025-04-05T17:39:09.758382Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras import regularizers\nimport keras_tuner as kt\nfrom tensorflow.keras import layers, models, regularizers\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras.metrics import Metric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:49:09.275176Z","iopub.execute_input":"2025-04-13T04:49:09.275496Z","iopub.status.idle":"2025-04-13T04:49:09.280797Z","shell.execute_reply.started":"2025-04-13T04:49:09.27547Z","shell.execute_reply":"2025-04-13T04:49:09.279767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set paths\nTRAIN_CSV = \"/kaggle/input/ai-vs-human-generated-dataset/train.csv\"\nTEST_CSV = \"/kaggle/input/ai-vs-human-generated-dataset/test.csv\"\nTRAIN_PATH = \"/kaggle/input/ai-vs-human-generated-dataset/train_data/\"\nTEST_PATH = \"/kaggle/input/ai-vs-human-generated-dataset/test_data_v2/\"\n\n# Load CSV files\ntrain_df = pd.read_csv(TRAIN_CSV)\ntest_df = pd.read_csv(TEST_CSV)\n\n# Check dataset\nprint(\"Train dataset shape:\", train_df.shape)\nprint(\"Test dataset shape:\", test_df.shape)\nprint(train_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:24.509131Z","iopub.execute_input":"2025-04-13T04:46:24.509628Z","iopub.status.idle":"2025-04-13T04:46:24.656265Z","shell.execute_reply.started":"2025-04-13T04:46:24.509595Z","shell.execute_reply":"2025-04-13T04:46:24.655388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:24.657684Z","iopub.execute_input":"2025-04-13T04:46:24.657936Z","iopub.status.idle":"2025-04-13T04:46:24.686284Z","shell.execute_reply.started":"2025-04-13T04:46:24.657916Z","shell.execute_reply":"2025-04-13T04:46:24.685455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:24.687468Z","iopub.execute_input":"2025-04-13T04:46:24.687771Z","iopub.status.idle":"2025-04-13T04:46:24.697382Z","shell.execute_reply.started":"2025-04-13T04:46:24.687749Z","shell.execute_reply":"2025-04-13T04:46:24.696235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop rows with missing values\ntrain_df = train_df.dropna().reset_index(drop=True)\ntest_df = test_df.dropna().reset_index(drop=True)\n\nprint(f\"After dropping NaN: Train shape: {train_df.shape}, Test shape: {test_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:24.698397Z","iopub.execute_input":"2025-04-13T04:46:24.698759Z","iopub.status.idle":"2025-04-13T04:46:24.722392Z","shell.execute_reply.started":"2025-04-13T04:46:24.698725Z","shell.execute_reply":"2025-04-13T04:46:24.721529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_df.columns)\nprint(test_df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:24.723186Z","iopub.execute_input":"2025-04-13T04:46:24.723441Z","iopub.status.idle":"2025-04-13T04:46:24.729407Z","shell.execute_reply.started":"2025-04-13T04:46:24.723411Z","shell.execute_reply":"2025-04-13T04:46:24.728434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract image paths and labels correctly\nDATASET_PATH = \"/kaggle/input/ai-vs-human-generated-dataset\"\n\ntrain_df[\"file_name\"] = train_df[\"file_name\"].apply(lambda x: os.path.join(DATASET_PATH, \"train_data\", os.path.basename(x)))\ntest_df[\"id\"] = test_df[\"id\"].apply(lambda x: os.path.join(DATASET_PATH, \"test_data_v2\", os.path.basename(x)))\n\n# Split into training and validation sets\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    train_df['file_name'].values, train_df['label'].values, test_size=0.2, random_state=42, stratify=train_df['label']\n)\n\nprint(f\"Training samples: {len(train_paths)}, Validation samples: {len(val_paths)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:24.730376Z","iopub.execute_input":"2025-04-13T04:46:24.730593Z","iopub.status.idle":"2025-04-13T04:46:24.951704Z","shell.execute_reply.started":"2025-04-13T04:46:24.730575Z","shell.execute_reply":"2025-04-13T04:46:24.950936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Any NaN in train paths?\", pd.isna(train_paths).sum())\nprint(\"Any NaN in train labels?\", pd.isna(train_labels).sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:25.350013Z","iopub.execute_input":"2025-04-13T04:46:25.350366Z","iopub.status.idle":"2025-04-13T04:46:25.364384Z","shell.execute_reply.started":"2025-04-13T04:46:25.350338Z","shell.execute_reply":"2025-04-13T04:46:25.363452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Image size required for EfficientNet\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 64\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# Function to load and preprocess images\ndef load_image(image_path, label=None):\n    # Load image\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)  # Ensure RGB\n    image = tf.image.resize(image, IMG_SIZE)  # Resize to EfficientNet input size  \n    image = preprocess_input(image)\n    # Return (image, label) for training, (image) for test\n    return (image, label) if label is not None else image\n\ndef gaussian_blur(image, kernel_size=5, sigma=1.0):\n    def _gaussian_kernel(size, sigma):\n        x = tf.range(-size // 2 + 1, size // 2 + 1, dtype=tf.float32)\n        x = tf.exp(-0.5 * tf.square(x) / tf.square(sigma))\n        kernel = tf.tensordot(x, x, axes=0)\n        kernel /= tf.reduce_sum(kernel)\n        return kernel[:, :, tf.newaxis, tf.newaxis]\n\n    kernel = _gaussian_kernel(kernel_size, sigma)\n    kernel = tf.repeat(kernel, repeats=3, axis=2)  # For RGB\n    image = tf.expand_dims(image, axis=0)  # Add batch dim\n    image = tf.nn.depthwise_conv2d(image, kernel, strides=[1, 1, 1, 1], padding='SAME')\n    return tf.squeeze(image, axis=0)  # Remove batch dim\n\ndef augment(image, label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.rot90(image)  # Flip 90 degrees\n    # image = tf.image.random_contrast(image, 0.8, 1.2)\n    image = tf.image.random_brightness(image, 0.2)\n    \n    # Randomly darken image\n    if tf.random.uniform([]) < 0.5:\n        image = tf.image.adjust_brightness(image, -0.2)\n    \n    # # Randomly apply blur\n    # if tf.random.uniform([]) < 0.3:\n    #     image = gaussian_blur(image)\n\n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:26.187265Z","iopub.execute_input":"2025-04-13T04:46:26.187589Z","iopub.status.idle":"2025-04-13T04:46:26.195227Z","shell.execute_reply.started":"2025-04-13T04:46:26.187562Z","shell.execute_reply":"2025-04-13T04:46:26.194366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create training dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\ntrain_dataset = train_dataset.map(load_image, num_parallel_calls=AUTOTUNE)\ntrain_dataset = train_dataset.map(augment, num_parallel_calls=AUTOTUNE)  # Augment training data\ntrain_dataset = train_dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\n# Create validation dataset (no augmentation)\nval_dataset = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\nval_dataset = val_dataset.map(load_image, num_parallel_calls=AUTOTUNE)\nval_dataset = val_dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\nprint(\"Datasets prepared successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:28.612058Z","iopub.execute_input":"2025-04-13T04:46:28.612418Z","iopub.status.idle":"2025-04-13T04:46:29.633285Z","shell.execute_reply.started":"2025-04-13T04:46:28.612393Z","shell.execute_reply":"2025-04-13T04:46:29.632457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Convert to numpy arrays if they're tensors\ntrain_class_labels = train_labels.numpy() if hasattr(train_labels, 'numpy') else train_labels\nval_class_labels = val_labels.numpy() if hasattr(val_labels, 'numpy') else val_labels\n\n# Set style\nsns.set(style=\"whitegrid\")\n\n# Create figure\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Plot training label distribution\nsns.countplot(x=train_class_labels, ax=axes[0], palette=\"coolwarm\")\naxes[0].set_title(\"Train Label Distribution\")\naxes[0].set_xlabel(\"Class (0 or 1)\")\naxes[0].set_ylabel(\"Count\")\naxes[0].set_xticks([0, 1])\n\n# Plot validation label distribution\nsns.countplot(x=val_class_labels, ax=axes[1], palette=\"coolwarm\")\naxes[1].set_title(\"Validation Label Distribution\")\naxes[1].set_xlabel(\"Class (0 or 1)\")\naxes[1].set_ylabel(\"Count\")\naxes[1].set_xticks([0, 1])\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:32.554459Z","iopub.execute_input":"2025-04-13T04:46:32.554769Z","iopub.status.idle":"2025-04-13T04:46:33.328082Z","shell.execute_reply.started":"2025-04-13T04:46:32.554747Z","shell.execute_reply":"2025-04-13T04:46:33.327165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Custom F1 Score Metric for training\n# class F1Score(Metric):\n#     def __init__(self, name='f1_score', **kwargs):\n#         super().__init__(name=name, **kwargs)\n#         self.true_positives = self.add_weight(name='tp', initializer='zeros')\n#         self.false_positives = self.add_weight(name='fp', initializer='zeros')\n#         self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n\n#     def update_state(self, y_true, y_pred, sample_weight=None):\n#         y_pred = tf.cast(tf.greater(y_pred, 0.5), tf.float32)\n#         y_true = tf.cast(y_true, tf.float32)\n\n#         tp = tf.reduce_sum(y_true * y_pred)\n#         fp = tf.reduce_sum((1 - y_true) * y_pred)\n#         fn = tf.reduce_sum(y_true * (1 - y_pred))\n\n#         self.true_positives.assign_add(tp)\n#         self.false_positives.assign_add(fp)\n#         self.false_negatives.assign_add(fn)\n\n#     def result(self):\n#         precision = self.true_positives / (self.true_positives + self.false_positives + 1e-6)\n#         recall = self.true_positives / (self.true_positives + self.false_negatives + 1e-6)\n#         return 2 * ((precision * recall) / (precision + recall + 1e-6))\n\n#     def reset_states(self):\n#         for var in self.variables:\n#             var.assign(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:34.983095Z","iopub.execute_input":"2025-04-13T04:46:34.9837Z","iopub.status.idle":"2025-04-13T04:46:34.987487Z","shell.execute_reply.started":"2025-04-13T04:46:34.983671Z","shell.execute_reply":"2025-04-13T04:46:34.98652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Model builder for Keras Tuner\n# def model_builder(hp):\n#     base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n#     base_model.trainable = True\n#     for layer in base_model.layers[:-30]:\n#         layer.trainable = False\n\n#     model = models.Sequential()\n#     model.add(base_model)\n#     model.add(layers.GlobalAveragePooling2D())\n\n#     # Tune number of dense layers\n#     for i in range(hp.Int(\"num_dense_layers\", 1, 3)):\n#         units = hp.Int(f\"dense_units_{i}\", min_value=64, max_value=512, step=64)\n#         activation = hp.Choice(\"activation\", [\"relu\", \"swish\", \"leaky_relu\", \"mish\"])\n\n#         if activation == \"leaky_relu\":\n#             model.add(layers.Dense(units, kernel_initializer=\"he_normal\"))\n#             model.add(layers.LeakyReLU(alpha=0.1))\n#         else:\n#             model.add(layers.Dense(units, activation=activation, kernel_initializer=\"he_normal\"))\n\n#         if hp.Boolean(f\"batchnorm_{i}\"):\n#             model.add(layers.BatchNormalization())\n\n#         if hp.Boolean(f\"dropout_{i}\"):\n#             model.add(layers.Dropout(hp.Float(f\"dropout_rate_{i}\", 0.3, 0.5)))\n\n#     model.add(layers.Dense(1, activation='sigmoid'))\n\n#     model.compile(\n#         optimizer=tf.keras.optimizers.AdamW(hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling='log')),\n#         loss=\"binary_crossentropy\",\n#         metrics=[F1Score()]\n#     )\n#     return model\n\n# trial_f1_log = {}\n    \n# # Callback to track F1 score on train & validation\n# class F1ScoreCallback(tf.keras.callbacks.Callback):\n#     def __init__(self, train_dataset, val_dataset, trial_id=None):\n#         super().__init__()\n#         self.train_dataset = train_dataset\n#         self.val_dataset = val_dataset\n#         self.trial_id = trial_id\n#         self.best_train_f1 = 0\n\n#     def on_epoch_end(self, epoch, logs=None):\n#         ...\n#         train_f1 = f1_score(train_true, train_preds)\n#         val_f1 = f1_score(val_true, val_preds)\n\n#         # Save best train F1\n#         self.best_train_f1 = max(self.best_train_f1, train_f1)\n\n#         print(f\"\\nEpoch {epoch + 1} - Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f}\")\n\n#     def on_train_end(self, logs=None):\n#         # Save best train F1 after all epochs\n#         trial_f1_log[self.trial_id] = self.best_train_f1\n\n# # Early stopping\n# early_stopping = tf.keras.callbacks.EarlyStopping(\n#     monitor=\"val_loss\",\n#     patience=3,\n#     restore_best_weights=True\n# )\n\n# # Keras Tuner setup\n# tuner = kt.Hyperband(\n#     model_builder,\n#     objective=\"val_loss\", \n#     max_epochs=10,\n#     factor=3,\n#     directory='keras_tuner_logs',\n#     project_name='f1_tuning_model'\n# )\n\n# # Manual training loop to track best TRAIN F1 across trials\n# trial_f1_log = {}\n\n# for trial in range(5):  # Run 5 hyperparameter trials (change as needed)\n#     # Sample a new hyperparameter set from the search space\n#     hp = tuner.oracle.get_space().copy()\n#     model = tuner.hypermodel.build(hp)\n\n#     trial_id = f\"trial_{trial}\"\n#     print(f\"\\n🔍 Running {trial_id}\")\n\n#     history = model.fit(\n#         train_dataset,\n#         validation_data=val_dataset,\n#         epochs=10,\n#         callbacks=[\n#             early_stopping,\n#             F1ScoreCallback(train_dataset, val_dataset, trial_id=trial_id)\n#         ]\n#     )\n\n# # Find best trial based on TRAIN F1 score\n# best_trial = max(trial_f1_log, key=trial_f1_log.get)\n# print(f\"\\n🏆 Best Trial: {best_trial} | Train F1: {trial_f1_log[best_trial]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:35.293756Z","iopub.execute_input":"2025-04-13T04:46:35.294087Z","iopub.status.idle":"2025-04-13T04:46:35.298541Z","shell.execute_reply.started":"2025-04-13T04:46:35.294063Z","shell.execute_reply":"2025-04-13T04:46:35.297723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nimport tensorflow as tf\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import EfficientNetB0\n\ndef build_model(trial):\n    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n    \n    # 🔧 Tune how many layers to unfreeze in the base model\n    unfreeze_layers = trial.suggest_int(\"unfreeze_layers\", 0, 30)\n    for layer in base_model.layers[:-unfreeze_layers]:\n        layer.trainable = False\n    for layer in base_model.layers[-unfreeze_layers:]:\n        layer.trainable = True\n\n    model = models.Sequential()\n    model.add(base_model)\n    model.add(layers.GlobalAveragePooling2D())\n\n    # 🔧 Tune number of dense layers\n    num_dense_layers = trial.suggest_int(\"num_dense_layers\", 1, 3)\n    for i in range(num_dense_layers):\n        units = trial.suggest_int(f\"dense_units_{i}\", 64, 1024, step=64)\n        activation = trial.suggest_categorical(f\"activation_{i}\", [\"relu\", \"swish\", \"leaky_relu\", \"mish\"])\n\n        model.add(layers.Dense(units, kernel_initializer=\"he_normal\"))\n        if activation == \"leaky_relu\":\n            model.add(layers.LeakyReLU(alpha=0.1))\n        elif activation == \"mish\":\n            model.add(tf.keras.layers.Activation(tf.nn.swish))  # Mish approximation\n        else:\n            model.add(layers.Activation(activation))\n\n        if trial.suggest_categorical(f\"batchnorm_{i}\", [True, False]):\n            model.add(layers.BatchNormalization())\n\n        if trial.suggest_categorical(f\"dropout_{i}\", [True, False]):\n            dropout_rate = trial.suggest_float(f\"dropout_rate_{i}\", 0.3, 0.5)\n            model.add(layers.Dropout(dropout_rate))\n\n    model.add(layers.Dense(1, activation='sigmoid'))\n    \n    return model\n\ndef objective(trial):\n    model = build_model(trial)\n    lr = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n\n    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n\n    # Train model (no callbacks for simplicity; you can add EarlyStopping)\n    history = model.fit(train_dataset, validation_data=val_dataset, epochs=5, verbose=0)\n\n    # Get validation predictions\n    y_true = np.concatenate([y for _, y in val_dataset], axis=0)\n    y_pred_prob = model.predict(val_dataset).flatten()\n    y_pred = (y_pred_prob > 0.5).astype(int)\n\n    f1 = f1_score(y_true, y_pred)\n\n    print(f\"Trial {trial.number}: F1 Score = {f1:.4f}\")\n    return f1\n\n# Run Optuna study\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=20)\n\n# Best trial result\nprint(\"\\n✅ Best Trial:\")\nprint(f\"F1 Score: {study.best_value:.4f}\")\nprint(\"Best Hyperparameters:\")\nfor k, v in study.best_trial.params.items():\n    print(f\"{k}: {v}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T16:12:32.894237Z","iopub.execute_input":"2025-04-12T16:12:32.895183Z","iopub.status.idle":"2025-04-12T20:16:39.644004Z","shell.execute_reply.started":"2025-04-12T16:12:32.895152Z","shell.execute_reply":"2025-04-12T20:16:39.643195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load EfficientNetB0 (Pretrained on ImageNet, without top layers)\nbase_model = EfficientNetB0(input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\")\n# base_model.trainable = False  # Freeze base model\n\nbase_model.trainable = True  # Unfreeze all layers\n\n# Freeze all layers except the last `unfreeze_layers`\nunfreeze_layers = 0  \nfor layer in base_model.layers[:-unfreeze_layers]:\n    layer.trainable = False\n\n# Build the model\nmodel = models.Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),  \n    \n    # Dense Layer 0\n    layers.Dense(640, kernel_initializer=\"he_normal\"),\n    layers.LeakyReLU(alpha=0.1),  # activation_0: leaky_relu\n    layers.BatchNormalization(),\n    layers.Dropout(0.3359853061113047),\n\n    # Dense Layer 1\n    layers.Dense(256, kernel_initializer=\"he_normal\", activation=\"mish\"),\n    layers.BatchNormalization(),\n\n    # Dense Layer 2\n    layers.Dense(512, activation='relu', kernel_initializer=\"he_normal\"),  # activation_2: relu\n\n    # Output\n    layers.Dense(1, activation='sigmoid')\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:43.740434Z","iopub.execute_input":"2025-04-13T04:46:43.740729Z","iopub.status.idle":"2025-04-13T04:46:46.759598Z","shell.execute_reply.started":"2025-04-13T04:46:43.740708Z","shell.execute_reply":"2025-04-13T04:46:46.758595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile the model\nmodel.compile(\n    optimizer=tf.keras.optimizers.AdamW(learning_rate=0.0005),\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\n# Print model summary\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:46.95593Z","iopub.execute_input":"2025-04-13T04:46:46.956266Z","iopub.status.idle":"2025-04-13T04:46:46.990594Z","shell.execute_reply.started":"2025-04-13T04:46:46.956243Z","shell.execute_reply":"2025-04-13T04:46:46.98991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define Callbacks\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint(\"best_model.keras\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\")\n\n# Train the Model\nhistory = model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=10,  # Adjust based on performance\n    callbacks=[early_stopping, model_checkpoint]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:49:40.99326Z","iopub.execute_input":"2025-04-13T04:49:40.993608Z","iopub.status.idle":"2025-04-13T05:07:03.905579Z","shell.execute_reply.started":"2025-04-13T04:49:40.993582Z","shell.execute_reply":"2025-04-13T05:07:03.90474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\n\n# Convert history to DataFrame for easy plotting\nhistory_df = pd.DataFrame(history.history)\n\n# Plot Loss Curve\nplt.figure(figsize=(10, 4))\nplt.plot(history_df[\"loss\"], label=\"Training Loss\")\nplt.plot(history_df[\"val_loss\"], label=\"Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss Curve\")\nplt.show()\n\n# Plot Accuracy Curve\nplt.figure(figsize=(10, 4))\nplt.plot(history_df[\"accuracy\"], label=\"Training Accuracy\")\nplt.plot(history_df[\"val_accuracy\"], label=\"Validation Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.title(\"Accuracy Curve\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:07:21.248404Z","iopub.execute_input":"2025-04-13T05:07:21.248733Z","iopub.status.idle":"2025-04-13T05:07:21.778521Z","shell.execute_reply.started":"2025-04-13T05:07:21.248709Z","shell.execute_reply":"2025-04-13T05:07:21.777732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 32\n\ndef decode_image(filename):\n    image = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(image, channels=3) # Adjust if images are PNG\n    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE]) # Resize to model input size\n    image = preprocess_input(image)  # Required for EfficientNet\n    return image\n    \n# Load test image paths\ntest_paths = test_df[\"id\"].values\n\n# Create a TensorFlow dataset for the test images\ntest_ds = tf.data.Dataset.from_tensor_slices(test_paths)\ntest_ds = test_ds.map(decode_image, num_parallel_calls=tf.data.AUTOTUNE)\ntest_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\nprint(\"Test dataset loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:07:38.197173Z","iopub.execute_input":"2025-04-13T05:07:38.1975Z","iopub.status.idle":"2025-04-13T05:07:38.257532Z","shell.execute_reply.started":"2025-04-13T05:07:38.197477Z","shell.execute_reply":"2025-04-13T05:07:38.256763Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(test_paths[:5])  # Print first 5 paths to verify\nprint(\"Any NaN in test paths?\", pd.isna(test_paths).sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:07:40.349613Z","iopub.execute_input":"2025-04-13T05:07:40.349944Z","iopub.status.idle":"2025-04-13T05:07:40.355666Z","shell.execute_reply.started":"2025-04-13T05:07:40.349916Z","shell.execute_reply":"2025-04-13T05:07:40.354864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to load and preprocess images (adjust based on your preprocessing)\n# def decode_image(filename, label):\n#     image = tf.io.read_file(filename)\n#     image = tf.image.decode_jpeg(image, channels=3)  # Adjust for your dataset format\n#     image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])  # Resize to model input size\n#     image = preprocess_input(image)  # Normalize \n#     return image, label\n    \n# # Create validation dataset\n# val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n# val_ds = val_ds.map(decode_image).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n# print(\"Validation dataset loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:26:16.889199Z","iopub.execute_input":"2025-04-08T08:26:16.889501Z","iopub.status.idle":"2025-04-08T08:26:16.951746Z","shell.execute_reply.started":"2025-04-08T08:26:16.889478Z","shell.execute_reply":"2025-04-08T08:26:16.950977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport random\n\n# View val sample\n# val_sample = random.choice(val_paths)\n# img = tf.keras.preprocessing.image.load_img(val_sample)\n# plt.imshow(img)\n# plt.title(\"Validation Image\")\n# plt.show()\n\n# View test sample\n# test_sample = random.choice(test_paths)\n# img = tf.keras.preprocessing.image.load_img(test_sample)\n# plt.imshow(img)\n# plt.title(\"Test Image\")\n# plt.show()\n\n# Pick 15 random samples\nrandom_samples = random.sample(list(test_paths), 15)\n\n# Create a 3x5 grid\nfig, axes = plt.subplots(3, 5, figsize=(15, 9))\n\nfor i, img_path in enumerate(random_samples):\n    img = tf.keras.preprocessing.image.load_img(img_path)\n    ax = axes[i // 5, i % 5]  # Determine row and column\n    ax.imshow(img)\n    ax.set_title(f\"Image {i+1}\")\n    ax.axis('off')  # Hide axis\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:07:53.944013Z","iopub.execute_input":"2025-04-13T05:07:53.944332Z","iopub.status.idle":"2025-04-13T05:07:58.586644Z","shell.execute_reply.started":"2025-04-13T05:07:53.94431Z","shell.execute_reply":"2025-04-13T05:07:58.58549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\ntrue_train_labels = []\nfor _, labels in train_dataset:\n    true_train_labels.extend(labels.numpy())\n\ntrue_val_labels = []\nfor _, labels in val_dataset:\n    true_val_labels.extend(labels.numpy())\n\n# Step 1: Get predictions on training set\ntrain_predictions = model.predict(train_dataset).flatten()\ntrain_preds_binary = (train_predictions > 0.5).astype(int)\n\n# Step 2: Get predictions on validation set\nval_predictions = model.predict(val_dataset).flatten()\nval_preds_binary = (val_predictions > 0.5).astype(int)\n\n# Step 3: Get true labels (you already have these)\n# Assuming train_labels and val_labels are available and are numpy arrays\n\n# Step 4: Calculate metrics\ntrain_precision = precision_score(true_train_labels, train_preds_binary)\ntrain_recall = recall_score(true_train_labels, train_preds_binary)\ntrain_f1 = f1_score(true_train_labels, train_preds_binary)\ntrain_accuracy = accuracy_score(true_train_labels, train_preds_binary)\n\nval_precision = precision_score(true_val_labels, val_preds_binary)\nval_recall = recall_score(true_val_labels, val_preds_binary)\nval_f1 = f1_score(true_val_labels, val_preds_binary)\nval_accuracy = accuracy_score(true_val_labels, val_preds_binary)\n\n# Step 5: Print results\nprint(\"=== Training Metrics ===\")\nprint(f\"Precision: {train_precision:.4f}\")\nprint(f\"Recall:    {train_recall:.4f}\")\nprint(f\"F1 Score:  {train_f1:.4f}\")\nprint(f\"Training Accuracy:   {train_accuracy:.4f}\")\n\nprint(\"\\n=== Validation Metrics ===\")\nprint(f\"Precision: {val_precision:.4f}\")\nprint(f\"Recall:    {val_recall:.4f}\")\nprint(f\"F1 Score:  {val_f1:.4f}\")\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:08:13.18315Z","iopub.execute_input":"2025-04-13T05:08:13.183474Z","iopub.status.idle":"2025-04-13T05:11:53.990995Z","shell.execute_reply.started":"2025-04-13T05:08:13.183449Z","shell.execute_reply":"2025-04-13T05:11:53.990213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.metrics import\n\n# # Convert model predictions (probabilities) to binary class labels (0 or 1)\n# train_preds_class = (train_predictions > 0.5).astype(int).flatten()\n# val_preds_class = (val_predictions > 0.5).astype(int).flatten()\n\n# # True labels (already binary and not one-hot encoded)\n# true_train_labels = train_labels.flatten()\n# true_val_labels = val_labels.flatten()\n\n# # Calculate accuracy\n# train_accuracy = accuracy_score(true_train_labels, train_preds_class)\n# val_accuracy = accuracy_score(true_val_labels, val_preds_class)\n\n# # Print results\n# print(\"=== Accuracy Metrics ===\")\n# print(f\"Training Accuracy:   {train_accuracy:.4f}\")\n# print(f\"Validation Accuracy: {val_accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Make sure predictions are binary class labels (0 or 1)\ntrain_preds_class = (train_predictions > 0.5).astype(int).flatten()\nval_preds_class = (val_predictions > 0.5).astype(int).flatten()\n\n# Set style\nsns.set(style=\"whitegrid\")\n\n# Create figure\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Plot predicted training labels\nsns.countplot(x=train_preds_binary, ax=axes[0], palette=\"mako\")\naxes[0].set_title(\"Predicted Train Label Distribution\")\naxes[0].set_xlabel(\"Predicted Class (0 or 1)\")\naxes[0].set_ylabel(\"Count\")\naxes[0].set_xticks([0, 1])\n\n# Plot predicted validation labels\nsns.countplot(x=val_preds_binary, ax=axes[1], palette=\"mako\")\naxes[1].set_title(\"Predicted Validation Label Distribution\")\naxes[1].set_xlabel(\"Predicted Class (0 or 1)\")\naxes[1].set_ylabel(\"Count\")\naxes[1].set_xticks([0, 1])\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:12:21.263216Z","iopub.execute_input":"2025-04-13T05:12:21.263538Z","iopub.status.idle":"2025-04-13T05:12:21.61205Z","shell.execute_reply.started":"2025-04-13T05:12:21.263513Z","shell.execute_reply":"2025-04-13T05:12:21.611154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get validation predictions\nval_predictions = model.predict(val_dataset).flatten()  # Ensure 1D array\nprint(f\"Min: {np.min(val_predictions)}, Max: {np.max(val_predictions)}, Mean: {np.mean(val_predictions)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:12:45.716852Z","iopub.execute_input":"2025-04-13T05:12:45.717202Z","iopub.status.idle":"2025-04-13T05:13:01.919494Z","shell.execute_reply.started":"2025-04-13T05:12:45.717177Z","shell.execute_reply":"2025-04-13T05:13:01.918665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make predictions\npredictions = model.predict(test_ds)\npredictions = predictions.flatten()  # Convert to 1D array\n\nprint(f\"Min: {np.min(predictions)}, Max: {np.max(predictions)}, Mean: {np.mean(predictions)}\")\n\n# Apply threshold\npredicted_labels = (predictions >= 0.07).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:16:43.192634Z","iopub.execute_input":"2025-04-13T05:16:43.193003Z","iopub.status.idle":"2025-04-13T05:17:18.072615Z","shell.execute_reply.started":"2025-04-13T05:16:43.19297Z","shell.execute_reply":"2025-04-13T05:17:18.0718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# How many 0s and 1s are predicted\nunique, counts = np.unique(predicted_labels, return_counts=True)\nprint(dict(zip(unique, counts)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:17:24.772406Z","iopub.execute_input":"2025-04-13T05:17:24.772717Z","iopub.status.idle":"2025-04-13T05:17:24.778071Z","shell.execute_reply.started":"2025-04-13T05:17:24.772696Z","shell.execute_reply":"2025-04-13T05:17:24.777255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.hist(val_predictions, bins=50, alpha=0.7, label='Validation')\nplt.hist(predictions, bins=50, alpha=0.7, label='Test')\nplt.legend()\nplt.title(\"Prediction Distribution\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:17:25.892851Z","iopub.execute_input":"2025-04-13T05:17:25.893201Z","iopub.status.idle":"2025-04-13T05:17:26.27984Z","shell.execute_reply.started":"2025-04-13T05:17:25.893173Z","shell.execute_reply":"2025-04-13T05:17:26.27904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a submission DataFrame\nsubmission = pd.DataFrame({\n    \"id\": test_df[\"id\"].apply(lambda x: \"/\".join(x.split(\"/\")[-2:])),  # Use actual test IDs from the dataset\n    \"label\": predicted_labels.flatten().astype(int)  # Convert predictions to integers\n})\n\n# Save as CSV without index and header\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index=False, encoding=\"utf-8\")\n\nprint(\"Predictions saved to 'submission.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:17:37.849802Z","iopub.execute_input":"2025-04-13T05:17:37.850171Z","iopub.status.idle":"2025-04-13T05:17:37.874521Z","shell.execute_reply.started":"2025-04-13T05:17:37.850143Z","shell.execute_reply":"2025-04-13T05:17:37.873785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/submission.csv\")\nprint(df.head())\nprint(df.dtypes)  # Check if id and label have correct types\ndf['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:17:39.09765Z","iopub.execute_input":"2025-04-13T05:17:39.098016Z","iopub.status.idle":"2025-04-13T05:17:39.116274Z","shell.execute_reply.started":"2025-04-13T05:17:39.097982Z","shell.execute_reply":"2025-04-13T05:17:39.115293Z"}},"outputs":[],"execution_count":null}]}